{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# study2 ëª¨ë¸. ì°¨ëŸ‰ íŒŒì† ì—¬ë¶€ íƒì§€\n",
        "\n",
        " ## 1. ë°ì´í„°ì…‹ êµ¬ì¶• ì›Œí¬í”Œë¡œìš° (Workflow)\n",
        " #### 1) ì¤€ë¹„ (Preparation)\n",
        "  - ì…ë ¥ ë°ì´í„°:\n",
        "    - Damaged í´ë” (íŒŒì† ì°¨ëŸ‰ ì´ë¯¸ì§€ + íŒŒì† ìœ„ì¹˜ ë¼ë²¨ JSON)\n",
        "    - Normal í´ë” (ì •ìƒ ì°¨ëŸ‰ ì´ë¯¸ì§€)\n",
        " - ë„êµ¬ (Tool):\n",
        "    - Study 1 ëª¨ë¸: ì´ë¯¸ í•™ìŠµëœ ì°¨ëŸ‰ íƒì§€ ëª¨ë¸ (yolov8_fine_tuning.pt)\n",
        " #### 2) ë°ì´í„° ë¶„í•  (Splitting)\n",
        "  - ë¹„ìœ¨: ì „ì²´ ë°ì´í„°ë¥¼ Train (70%) : Val (20%) : Test (10%) ë¹„ìœ¨ë¡œ ë¬´ì‘ìœ„ ë¶„í• \n",
        "  - ëª©ì : í•™ìŠµìš©, ê²€ì¦ìš©, í‰ê°€ìš© ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë‚˜ëˆ„ì–´ ë°ì´í„° ìœ ì¶œ(Data Leakage)ì„ ë°©ì§€\n",
        " #### 3) í•µì‹¬ ì²˜ë¦¬ ê³¼ì • (Core Process) - ë°˜ë³µë¬¸(Loop)\n",
        "  - ì°¨ëŸ‰ íƒì§€ (Detect Vehicle):\n",
        "    - 1ë‹¨ê³„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ **ì°¨ëŸ‰ì˜ìœ„ì¹˜BBox**ë¥¼ ì°¾ìŒ\n",
        "    - (ë§Œì•½ ì°¨ëŸ‰ì„ ëª» ì°¾ìœ¼ë©´? â†’ ì›ë³¸ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì‚¬ìš©í•˜ë„ë¡ Fallback)\n",
        " - ì˜ì—­ ìë¥´ê¸° (Crop with Margin):\n",
        "    - ì°¨ëŸ‰ BBoxì— **ì—¬ìœ ê³µê°„(Margin15%)** ì„ ë”í•´ì„œ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ëƒ„.\n",
        "    - ì´ë•Œ, ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ì¢Œí‘œëŠ” **Clamp(ìš¸íƒ€ë¦¬)** ì²˜ë¦¬í•˜ì—¬ ì´ë¯¸ì§€ ì•ˆìª½ìœ¼ë¡œ ë§ì¶¤\n",
        " - í¬ê¸° ì¡°ì • (Resize):\n",
        "    - ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ í¬ê¸°(ì˜ˆ: 640x640 ë˜ëŠ” 1024x1024)ë¡œ ë³€ê²½\n",
        " - ë¼ë²¨ ë³€í™˜ (Label Remapping):\n",
        "    - Damaged: ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ì˜ íŒŒì† ì¢Œí‘œ (x, y)ë¥¼ Cropëœ ì´ë¯¸ì§€ ê¸°ì¤€ì˜ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜.(ì´ë•Œë„ ì¢Œí‘œê°€ íŠ€ì§€ ì•Šê²Œ Clamp ì ìš© í•„ìˆ˜)\n",
        "    - Normal: íŒŒì†ì´ ì—†ìœ¼ë¯€ë¡œ **ë¹ˆ í…ìŠ¤íŠ¸ íŒŒì¼.txt**ì„ ìƒì„± (Negative Sample í•™ìŠµìš©)\n",
        " #### 4) ì €ì¥ (Saving)\n",
        " - ë³€í™˜ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨(.txt)ì„ YOLO í•™ìŠµ í¬ë§·ì— ë§ì¶° ì €ì¥\n",
        "    - images/train,\n",
        "    - labels/trainimages/val,\n",
        "    - labels/valimages/test, labels/test\n",
        " - **í´ë˜ìŠ¤ í†µí•©**: ë°ì´í„° ë³µì¡ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë“  íŒŒì† ì¢…ë¥˜(Scratch, Dent ë“±)ë¥¼ 'Damage' (ID: 0) í•˜ë‚˜ë¡œ í†µí•©\n",
        " #### 5) ê²€ì¦ (Verification)\n",
        " - ì‹œê°í™”: ë³€í™˜ëœ ë°ì´í„°(Test Set) ì¤‘ 3ì¥ì„ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„, íŒŒì† ë°•ìŠ¤ê°€ ì°¨ëŸ‰ ìœ„ì— ì •í™•íˆ ê·¸ë ¤ì¡ŒëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸"
      ],
      "metadata": {
        "id": "n81wrpUcfmvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„°ì…‹ êµ¬ì¶• ë° ë¶„í• "
      ],
      "metadata": {
        "id": "Ma4uNNe4pqFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics tqdm\n",
        "\n",
        "# [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import json\n",
        "import random\n",
        "import cv2\n",
        "import yaml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
        "# =========================================================\n",
        "# 1. ì›ë³¸ ë°ì´í„° ê²½ë¡œ\n",
        "SOURCE_ROOT = \"/content/drive/MyDrive/03. HDMF/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing\"\n",
        "\n",
        "# 2. ì €ì¥í•  ë°ì´í„°ì…‹ ê²½ë¡œ\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/CAR_DAMAGE_DETECTION_CLASS_ALL\"\n",
        "\n",
        "# 3. 1ë‹¨ê³„ ì°¨ëŸ‰ íƒì§€ ëª¨ë¸ ê²½ë¡œ (â˜… ì¤‘ìš”: ì‹¤ì œ pt íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”)\n",
        "# ì˜ˆ: .../runs/detect/train/weights/best.pt\n",
        "MODEL_PATH = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK1_CAR_DETECTION/FINE_TUNING_MODEL/yolov8m_fine_tuning_3rd/weights/best.pt\"\n",
        "\n",
        "# 4. íŒŒë¼ë¯¸í„°\n",
        "IMG_SIZE = 640       # 2ë‹¨ê³„ ëª¨ë¸ ì…ë ¥ ì‚¬ì´ì¦ˆ (ë¦¬ì‚¬ì´ì§•)\n",
        "MARGIN_RATIO = 0.15  # ì°¨ëŸ‰ Crop ì‹œ ì—¬ìœ  ê³µê°„ (15%)\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"âœ… ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "5MCN-AV9gMNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ (Clamp, ì¢Œí‘œ ë³€í™˜)\n",
        "\n",
        "def clamp(value, min_val, max_val):\n",
        "    \"\"\"ê°’ì„ [min_val, max_val] ë²”ìœ„ ë‚´ë¡œ ê³ ì • (Clamp ë¡œì§)\"\"\"\n",
        "    return max(min_val, min(value, max_val))\n",
        "\n",
        "def get_vehicle_crop_box(image, model, margin=0.15):\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ì—ì„œ ì°¨ëŸ‰ì„ íƒì§€í•˜ê³ , Marginì„ í¬í•¨í•œ Crop ì¢Œí‘œ(x1, y1, x2, y2)ë¥¼ ë°˜í™˜.\n",
        "    íƒì§€ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì´ë¯¸ì§€ ì¢Œí‘œ ë°˜í™˜ (Fallback).\n",
        "    \"\"\"\n",
        "    h_img, w_img = image.shape[:2]\n",
        "\n",
        "    # 1ë‹¨ê³„ ëª¨ë¸ ì¶”ë¡ \n",
        "    results = model(image, verbose=False, conf=0.25)\n",
        "\n",
        "    best_box = None\n",
        "    max_area = 0\n",
        "\n",
        "    # ê°€ì¥ í° ì°¨ëŸ‰ ë°•ìŠ¤ ì„ íƒ (ì—¬ëŸ¬ ëŒ€ì¼ ê²½ìš° ë©”ì¸ ì°¨ëŸ‰ íƒ€ê²ŸíŒ…)\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "            # box.xyxy: [x1, y1, x2, y2]\n",
        "            coords = box.xyxy[0].cpu().numpy()\n",
        "            area = (coords[2] - coords[0]) * (coords[3] - coords[1])\n",
        "            if area > max_area:\n",
        "                max_area = area\n",
        "                best_box = coords\n",
        "\n",
        "    if best_box is None:\n",
        "        # Fallback: ì°¨ëŸ‰ ë¯¸íƒì§€ ì‹œ ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš© (ì¤‘ì•™ Crop íš¨ê³¼ ë“±ì„ ìœ„í•´)\n",
        "        return 0, 0, w_img, h_img, False # False indicates detection failed\n",
        "\n",
        "    # Margin ì ìš©\n",
        "    x1, y1, x2, y2 = best_box\n",
        "    w_box = x2 - x1\n",
        "    h_box = y2 - y1\n",
        "\n",
        "    m_w = w_box * margin\n",
        "    m_h = h_box * margin\n",
        "\n",
        "    # ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ Clamp ì ìš©\n",
        "    nx1 = int(clamp(x1 - m_w, 0, w_img))\n",
        "    ny1 = int(clamp(y1 - m_h, 0, h_img))\n",
        "    nx2 = int(clamp(x2 + m_w, 0, w_img))\n",
        "    ny2 = int(clamp(y2 + m_h, 0, h_img))\n",
        "\n",
        "    return nx1, ny1, nx2, ny2, True\n",
        "\n",
        "def process_and_save(src_img_path, src_json_path, dst_img_path, dst_txt_path, model):\n",
        "    \"\"\"ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì €ì¥\"\"\"\n",
        "    img = cv2.imread(src_img_path)\n",
        "    if img is None: return False\n",
        "\n",
        "    h_org, w_org = img.shape[:2]\n",
        "\n",
        "    # 1. ì°¨ëŸ‰ Crop ì¢Œí‘œ ê³„ì‚°\n",
        "    cx1, cy1, cx2, cy2, detected = get_vehicle_crop_box(img, model, MARGIN_RATIO)\n",
        "    crop_w = cx2 - cx1\n",
        "    crop_h = cy2 - cy1\n",
        "\n",
        "    if crop_w <= 0 or crop_h <= 0: return False # ì˜ˆì™¸ ì²˜ë¦¬\n",
        "\n",
        "    # 2. ì´ë¯¸ì§€ Crop & Resize\n",
        "    crop_img = img[cy1:cy2, cx1:cx2]\n",
        "    resized_img = cv2.resize(crop_img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # 3. ë¼ë²¨ ì²˜ë¦¬ (Normalì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸, DamagedëŠ” JSON íŒŒì‹±)\n",
        "    yolo_labels = []\n",
        "\n",
        "    if src_json_path and os.path.exists(src_json_path):\n",
        "        with open(src_json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for ann in data.get('annotations', []):\n",
        "            # JSON format: bbox [x_min, y_min, width, height] (Global)\n",
        "            gx, gy, gw, gh = ann['bbox']\n",
        "\n",
        "            # Global xyxy\n",
        "            gx1, gy1 = gx, gy\n",
        "            gx2, gy2 = gx + gw, gy + gh\n",
        "\n",
        "            # Remapping: Global -> Crop Local\n",
        "            lx1 = gx1 - cx1\n",
        "            ly1 = gy1 - cy1\n",
        "            lx2 = gx2 - cx1\n",
        "            ly2 = gy2 - cy1\n",
        "\n",
        "            # Clamp Logic: Crop ì˜ì—­ ë°–ì˜ íŒŒì† ë¶€ìœ„ ì˜ë¼ë‚´ê¸°\n",
        "            lx1 = clamp(lx1, 0, crop_w)\n",
        "            ly1 = clamp(ly1, 0, crop_h)\n",
        "            lx2 = clamp(lx2, 0, crop_w)\n",
        "            ly2 = clamp(ly2, 0, crop_h)\n",
        "\n",
        "            # ìœ íš¨ì„± ê²€ì‚¬ (ë„ˆë¹„ë‚˜ ë†’ì´ê°€ ë„ˆë¬´ ì‘ì•„ì¡Œìœ¼ë©´ ìŠ¤í‚µ)\n",
        "            lw = lx2 - lx1\n",
        "            lh = ly2 - ly1\n",
        "            if lw < 1 or lh < 1:\n",
        "                continue\n",
        "\n",
        "            # YOLO Format Conversion (Normalized 0~1)\n",
        "            # Resize í–ˆìœ¼ë¯€ë¡œ ë¹„ìœ¨ì€ Crop ì´ë¯¸ì§€ ê¸°ì¤€ê³¼ ë™ì¼\n",
        "            x_center = (lx1 + lw / 2) / crop_w\n",
        "            y_center = (ly1 + lh / 2) / crop_h\n",
        "            w_norm = lw / crop_w\n",
        "            h_norm = lh / crop_h\n",
        "\n",
        "            # Class ID: 0 (Damage í†µí•©)\n",
        "            yolo_labels.append(f\"0 {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
        "\n",
        "    # 4. ì €ì¥\n",
        "    cv2.imwrite(dst_img_path, resized_img)\n",
        "\n",
        "    with open(dst_txt_path, 'w') as f:\n",
        "        f.write(\"\\n\".join(yolo_labels))\n",
        "\n",
        "    return True\n",
        "\n",
        "print(\"âœ… í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "mQa9JAcVh27-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„°ì…‹ êµ¬ì¶• ë©”ì¸ í”„ë¡œì„¸ìŠ¤ (ë°˜ë³µ x)"
      ],
      "metadata": {
        "id": "SzVdZyG9pyI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 3] ë°ì´í„°ì…‹ êµ¬ì¶• ë©”ì¸ í”„ë¡œì„¸ìŠ¤\n",
        "def build_dataset():\n",
        "    # 0. ë””ë ‰í† ë¦¬ ì´ˆê¸°í™” ë° ìƒì„±\n",
        "    if os.path.exists(DATASET_ROOT):\n",
        "        shutil.rmtree(DATASET_ROOT)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(DATASET_ROOT, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(DATASET_ROOT, 'labels', split), exist_ok=True)\n",
        "\n",
        "    # 1. ëª¨ë¸ ë¡œë“œ\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"âŒ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {MODEL_PATH}\")\n",
        "        return\n",
        "    print(f\"ğŸš€ ëª¨ë¸ ë¡œë“œ ì¤‘: {os.path.basename(MODEL_PATH)}\")\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 2. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
        "    print(\"ğŸ“‚ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...\")\n",
        "\n",
        "    # Damaged Data (Positive)\n",
        "    damaged_img_dir = os.path.join(SOURCE_ROOT, \"damaged\", \"images\")\n",
        "    damaged_lbl_dir = os.path.join(SOURCE_ROOT, \"damaged\", \"labels\")\n",
        "    damaged_files = glob.glob(os.path.join(damaged_img_dir, \"*.jpg\")) + \\\n",
        "                    glob.glob(os.path.join(damaged_img_dir, \"*.png\"))\n",
        "\n",
        "    # Normal Data (Negative)\n",
        "    normal_img_dir = os.path.join(SOURCE_ROOT, \"normal\", \"images\")\n",
        "    normal_files = glob.glob(os.path.join(normal_img_dir, \"*.jpg\")) + \\\n",
        "                   glob.glob(os.path.join(normal_img_dir, \"*.png\"))\n",
        "\n",
        "    # 3. ë°ì´í„° ë¶„í•  (7:2:1)\n",
        "    def split_data(file_list):\n",
        "        train, test = train_test_split(file_list, test_size=0.1, random_state=SEED)\n",
        "        train, val = train_test_split(train, test_size=0.2222, random_state=SEED) # 0.9 * 0.2222 â‰ˆ 0.2\n",
        "        return train, val, test\n",
        "\n",
        "    d_train, d_val, d_test = split_data(damaged_files)\n",
        "    n_train, n_val, n_test = split_data(normal_files)\n",
        "\n",
        "    # 4. ì²˜ë¦¬ ë£¨í”„\n",
        "    tasks = [\n",
        "        ('train', d_train, 'damaged'), ('val', d_val, 'damaged'), ('test', d_test, 'damaged'),\n",
        "        ('train', n_train, 'normal'),  ('val', n_val, 'normal'),  ('test', n_test, 'normal')\n",
        "    ]\n",
        "\n",
        "    total_stats = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "    print(\"âš¡ ë°ì´í„° ì „ì²˜ë¦¬ ë° Crop ì‹œì‘ (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë©ë‹ˆë‹¤)...\")\n",
        "\n",
        "    for split, files, kind in tasks:\n",
        "        for file_path in tqdm(files, desc=f\"[{split}] {kind}\"):\n",
        "            filename = os.path.basename(file_path)\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "\n",
        "            dst_img = os.path.join(DATASET_ROOT, 'images', split, filename)\n",
        "            dst_txt = os.path.join(DATASET_ROOT, 'labels', split, basename + \".txt\")\n",
        "\n",
        "            json_path = None\n",
        "            if kind == 'damaged':\n",
        "                json_path = os.path.join(damaged_lbl_dir, basename + \".json\")\n",
        "\n",
        "            success = process_and_save(file_path, json_path, dst_img, dst_txt, model)\n",
        "            if success:\n",
        "                total_stats[split] += 1\n",
        "\n",
        "    # 5. YAML íŒŒì¼ ìƒì„±\n",
        "    yaml_content = f\"\"\"\n",
        "path: {DATASET_ROOT}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "nc: 1\n",
        "names: ['Damage']\n",
        "\"\"\"\n",
        "    with open(os.path.join(DATASET_ROOT, \"data_damage.yaml\"), \"w\") as f:\n",
        "        f.write(yaml_content)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± í˜„í™©\")\n",
        "    print(f\"   Train : {total_stats['train']} ì¥\")\n",
        "    print(f\"   Val   : {total_stats['val']} ì¥\")\n",
        "    print(f\"   Test  : {total_stats['test']} ì¥\")\n",
        "    print(f\"   Total : {sum(total_stats.values())} ì¥\")\n",
        "    print(f\"   YAML  : {os.path.join(DATASET_ROOT, 'data_damage.yaml')}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "build_dataset()"
      ],
      "metadata": {
        "id": "yX3f6Obajq1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì‹œê°í™” (ê²€ì¦)"
      ],
      "metadata": {
        "id": "woQKgyZXp8RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 4] ì‹œê°í™” (ê²€ì¦)\n",
        "def visualize_random_samples(num_samples=3):\n",
        "    \"\"\"\n",
        "    ìƒì„±ëœ ë°ì´í„°ì…‹(Test Set) ì¤‘ Damaged ë¼ë²¨ì´ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ê³¨ë¼ ì‹œê°í™”\n",
        "    \"\"\"\n",
        "    test_img_dir = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    test_lbl_dir = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    # 1. ë¼ë²¨ íŒŒì¼ì´ ìˆëŠ”(ë‚´ìš©ì´ ìˆëŠ”) íŒŒì¼ ì°¾ê¸°\n",
        "    candidates = []\n",
        "    for txt_file in os.listdir(test_lbl_dir):\n",
        "        txt_path = os.path.join(test_lbl_dir, txt_file)\n",
        "        if os.path.getsize(txt_path) > 0: # ë‚´ìš©ì´ ìˆìœ¼ë©´ Damageê°€ ìˆëŠ” ê²ƒ\n",
        "            candidates.append(txt_file)\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"ì‹œê°í™”í•  íŒŒì† ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # 2. ëœë¤ ìƒ˜í”Œë§\n",
        "    samples = random.sample(candidates, min(len(candidates), num_samples))\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, txt_name in enumerate(samples):\n",
        "        basename = os.path.splitext(txt_name)[0]\n",
        "        img_name = basename + \".jpg\" # í™•ì¥ìëŠ” jpgë¼ê³  ê°€ì • (ë˜ëŠ” í™•ì¸ í•„ìš”)\n",
        "        if not os.path.exists(os.path.join(test_img_dir, img_name)):\n",
        "            img_name = basename + \".png\"\n",
        "\n",
        "        img_path = os.path.join(test_img_dir, img_name)\n",
        "        txt_path = os.path.join(test_lbl_dir, txt_name)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
        "        with open(txt_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "\n",
        "                # YOLO -> Pixel\n",
        "                x1 = int((cx - bw/2) * w)\n",
        "                y1 = int((cy - bh/2) * h)\n",
        "                x2 = int((cx + bw/2) * w)\n",
        "                y2 = int((cy + bh/2) * h)\n",
        "\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                cv2.putText(img, \"Damage\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Sample {i+1}\\n{basename}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ì‹¤í–‰\n",
        "print(\"ğŸ” ìƒì„±ëœ ë°ì´í„°ì…‹ ê²€ì¦ (Random 3 Samples)\")"
      ],
      "metadata": {
        "id": "FxzeBgwsj1FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_random_samples()"
      ],
      "metadata": {
        "id": "Lae9tlO9qB6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### trainì‹œ 1ì‹œê°„ ì •ë„ ì†Œëª¨ ë©ë‹ˆë‹¤. colabí™˜ê²½ì—ì„œ ì•„ë˜ë¥¼ browserì˜ console ì—ì„œ ë¶™ì—¬ ë„£ê¸°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "### shift+cntr+i ë¡œ browser console ì—´ê¸°\n",
        "# https://github.com/chulminkw/DLCV/blob/master/data/util/colab_autoclick.js\n",
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "metadata": {
        "id": "MqRu4hO7vJ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ 1ì°¨"
      ],
      "metadata": {
        "id": "csMfMuslqJTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í•™ìŠµ ì„¤ì •\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (Step 2ì—ì„œ ìƒì„±í•œ ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•¨)\n",
        "# =========================================================\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/CAR_DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage.yaml')\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ì´ˆê¸°í™”\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"TRAIN_RESULT\")\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "print(f\"ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: {DATASET_ROOT}\")\n",
        "print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "zdmnfkX1u4EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] YOLOv8 ëª¨ë¸ í•™ìŠµ (Training)\n",
        "\n",
        "def train_model():\n",
        "    # 1. ëª¨ë¸ ë¡œë“œ (íŒŒì†ì€ ë¯¸ì„¸í•œ íŠ¹ì§•ì´ ì¤‘ìš”í•˜ë¯€ë¡œ 'm' ëª¨ë¸ ê¶Œì¥)\n",
        "    # ê¸°ì¡´ pt íŒŒì¼ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ë¡œë“œí•´ì„œ Fine-tuning, ì—†ë‹¤ë©´ yolov8m.pt ë‹¤ìš´ë¡œë“œ\n",
        "    model = YOLO('yolov8m.pt')\n",
        "\n",
        "    print(\"ğŸ”¥ í•™ìŠµ ì‹œì‘ (Damage Detection)...\")\n",
        "\n",
        "    # 2. í•™ìŠµ ì‹¤í–‰\n",
        "    model.train(\n",
        "        data=YAML_PATH,\n",
        "        epochs=100,            # 50 Epoch (ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì ˆ)\n",
        "        imgsz=640,            # Cropëœ ì´ë¯¸ì§€ì´ë¯€ë¡œ 640ì´ë©´ ì¶©ë¶„\n",
        "        batch=16,\n",
        "        patience=10,          # 10ë²ˆ ë™ì•ˆ ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ\n",
        "        project=SAVE_DIR,     # ì €ì¥í•  í”„ë¡œì íŠ¸ í´ë”\n",
        "        name='yolov8m_damage_class_all_1st',# ì €ì¥í•  í•˜ìœ„ í´ë” ì´ë¦„\n",
        "        exist_ok=True,\n",
        "        optimizer='AdamW',\n",
        "        lr0=1e-4,             # Fine-tuningì´ë¯€ë¡œ í•™ìŠµë¥ ì„ ì¡°ê¸ˆ ë‚®ê²Œ ì„¤ì •\n",
        "        close_mosaic=10,      # ë§ˆì§€ë§‰ 10 epochì€ Mosaic Augmentation ë„ê¸° (ì •ë°€ë„ í–¥ìƒ)\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    return os.path.join(SAVE_DIR, 'yolov8m_damage', 'weights', 'best.pt')\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "best_model_path = train_model()\n",
        "print(f\"ğŸ† Best Model Path: {best_model_path}\")"
      ],
      "metadata": {
        "id": "_4qyGiQivaUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(test)"
      ],
      "metadata": {
        "id": "jmpOc4oO0U9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/TRAIN_RESULT/yolov8m_damage_class_all_1st/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st_025.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ],
      "metadata": {
        "id": "5e52IFvev9c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ],
      "metadata": {
        "id": "tiPibN95zJhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)\n",
        "# =========================================================\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/CAR_DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"TRAIN_RESULT\")\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "\n",
        "\n",
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_class_all_1st', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st_025.csv\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë° ë¼ë²¨ ê²½ë¡œ\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "def visualize_failures_batch(start_index=0, batch_size=3):\n",
        "    \"\"\"\n",
        "    ì˜¤íƒ ë°ì´í„°ë¥¼ start_indexë¶€í„° batch_sizeë§Œí¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # 1. ê²°ê³¼ CSV ë¡œë“œ ë° ì˜¤íƒ í•„í„°ë§\n",
        "    if not os.path.exists(RESULT_CSV_PATH):\n",
        "        print(f\"âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RESULT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(RESULT_CSV_PATH)\n",
        "    failures = df[df['is_correct'] == False] # ì˜¤íƒë§Œ ì¶”ì¶œ\n",
        "\n",
        "    total_failures = len(failures)\n",
        "    if total_failures == 0:\n",
        "        print(\"ğŸ‰ ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! ì™„ë²½í•©ë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬\n",
        "    if start_index >= total_failures:\n",
        "        print(f\"âš ï¸ ë” ì´ìƒ í™•ì¸í•  ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_failures}ê±´)\")\n",
        "        return\n",
        "\n",
        "    end_index = min(start_index + batch_size, total_failures)\n",
        "    target_samples = failures.iloc[start_index : end_index]\n",
        "\n",
        "    print(f\"ğŸ” ì˜¤íƒ ë°ì´í„° ì‹œê°í™”: {start_index+1} ~ {end_index} / ì´ {total_failures}ê±´\")\n",
        "\n",
        "    # 2. ëª¨ë¸ ë¡œë“œ (ë°•ìŠ¤ ì‹œê°í™”ìš©)\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 3. ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 6 * len(target_samples)))\n",
        "\n",
        "    for i, (_, row) in enumerate(target_samples.iterrows()):\n",
        "        filename = row['filename']\n",
        "        true_label = row['true_label'] # 0: Normal, 1: Damaged\n",
        "        img_path = os.path.join(TEST_IMG_DIR, filename)\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # A. ì •ë‹µ(Ground Truth) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    x1 = int((cx - bw/2) * w)\n",
        "                    y1 = int((cy - bh/2) * h)\n",
        "                    x2 = int((cx + bw/2) * w)\n",
        "                    y2 = int((cy + bh/2) * h)\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "                    cv2.putText(img, \"GT\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        # B. ëª¨ë¸ ì˜ˆì¸¡(Prediction) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "        # ì‹œê°í™”ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì¶”ë¡  ìˆ˜í–‰\n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                cv2.putText(img, f\"Pred {conf:.2f}\", (x1, y2+25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "        # íƒ€ì´í‹€ ì„¤ì •\n",
        "        str_true = \"Damaged\" if true_label == 1 else \"Normal\"\n",
        "        str_pred = \"Normal\" if true_label == 1 else \"Damaged\" # ì˜¤íƒì´ë¯€ë¡œ ë°˜ëŒ€\n",
        "        fail_type = \"False Negative (ë¯¸íƒ)\" if true_label == 1 else \"False Positive (ì˜¤íƒ/ê³¼íƒ)\"\n",
        "\n",
        "        plt.subplot(len(target_samples), 1, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"[{start_index+i+1}] {filename}\\nType: {fail_type} | GT: {str_true} (Green) vs Pred: {str_pred} (Red)\",\n",
        "                  fontsize=14, color='red', fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "O-RxAhZ723TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_class_all_1st', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st_025.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=20)"
      ],
      "metadata": {
        "id": "J9cs6eAn3WjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼)\n",
        "# =========================================================\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/CAR_DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"TRAIN_RESULT\")\n",
        "\n",
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_class_all_1st', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st.csv\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë° ë¼ë²¨ ê²½ë¡œ\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "def visualize_tp_batch(start_index=0, batch_size=3):\n",
        "    \"\"\"\n",
        "    ì •íƒ(True Positive) ë°ì´í„°ë¥¼ start_indexë¶€í„° batch_sizeë§Œí¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # 1. ê²°ê³¼ CSV ë¡œë“œ\n",
        "    if not os.path.exists(RESULT_CSV_PATH):\n",
        "        print(f\"âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RESULT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(RESULT_CSV_PATH)\n",
        "\n",
        "    # 2. TP í•„í„°ë§ (ì •ë‹µì´ Damaged(1)ì´ê³ , ì˜ˆì¸¡ë„ Damaged(1)ì¸ ê²½ìš°)\n",
        "    # is_correctê°€ Trueì´ë©´ì„œ true_labelì´ 1ì¸ ê²ƒ\n",
        "    tp_cases = df[(df['is_correct'] == True) & (df['true_label'] == 1)]\n",
        "\n",
        "    total_tp = len(tp_cases)\n",
        "    if total_tp == 0:\n",
        "        print(\"âš ï¸ ì •íƒ(True Positive) ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬\n",
        "    if start_index >= total_tp:\n",
        "        print(f\"âš ï¸ ë” ì´ìƒ í™•ì¸í•  TP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_tp}ê±´)\")\n",
        "        return\n",
        "\n",
        "    end_index = min(start_index + batch_size, total_tp)\n",
        "    target_samples = tp_cases.iloc[start_index : end_index]\n",
        "\n",
        "    print(f\"âœ… ì •íƒ(TP) ë°ì´í„° ì‹œê°í™”: {start_index+1} ~ {end_index} / ì´ {total_tp}ê±´\")\n",
        "\n",
        "    # 3. ëª¨ë¸ ë¡œë“œ\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 4. ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 6 * len(target_samples)))\n",
        "\n",
        "    for i, (_, row) in enumerate(target_samples.iterrows()):\n",
        "        filename = row['filename']\n",
        "        img_path = os.path.join(TEST_IMG_DIR, filename)\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # A. ì •ë‹µ(Ground Truth) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    x1 = int((cx - bw/2) * w)\n",
        "                    y1 = int((cy - bh/2) * h)\n",
        "                    x2 = int((cx + bw/2) * w)\n",
        "                    y2 = int((cy + bh/2) * h)\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "                    cv2.putText(img, \"GT\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        # B. ëª¨ë¸ ì˜ˆì¸¡(Prediction) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                cv2.putText(img, f\"Pred {conf:.2f}\", (x1, y2+25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "        # íƒ€ì´í‹€ ì„¤ì •\n",
        "        plt.subplot(len(target_samples), 1, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"[{start_index+i+1}] {filename}\\nType: True Positive (ì •íƒ) | GT: Damaged (Green) vs Pred: Damaged (Red)\",\n",
        "                  fontsize=14, color='green', fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ppsQ-AmQ3W7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì‹¤í–‰: ì²« ë²ˆì§¸ 3ì¥ í™•ì¸\n",
        "# =========================================================\n",
        "visualize_tp_batch(start_index=9, batch_size=3)"
      ],
      "metadata": {
        "id": "9yhHqQZT4Xq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25->0.1)"
      ],
      "metadata": {
        "id": "gTxfYqL95mQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/TRAIN_RESULT/yolov8m_damage_class_all_1st/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.1, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st_010.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ],
      "metadata": {
        "id": "zrIrK-FA4YBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_class_all_1st', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_1st_010.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=13)"
      ],
      "metadata": {
        "id": "p6NJcCl55sn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "deAAXxsF6mgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ 2ì°¨\n",
        " - IMG_SIZE = 1024\n",
        " - model = yolov8x\n",
        " - close_mosaic=15"
      ],
      "metadata": {
        "id": "CQver0mm6t8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í•™ìŠµ ì„¤ì •\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (Step 2ì—ì„œ ìƒì„±í•œ ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•¨)\n",
        "# =========================================================\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/CAR_DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage.yaml')\n",
        "\n",
        "# ì €ì¥ ê²½ë¡œ ì´ˆê¸°í™”\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"TRAIN_RESULT\")\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "print(f\"ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: {DATASET_ROOT}\")\n",
        "print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "n9kQpuVP6t8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] YOLOv8 ëª¨ë¸ í•™ìŠµ (Training)\n",
        "\n",
        "def train_model():\n",
        "    # 1. ëª¨ë¸ ë¡œë“œ (íŒŒì†ì€ ë¯¸ì„¸í•œ íŠ¹ì§•ì´ ì¤‘ìš”í•˜ë¯€ë¡œ 'm' ëª¨ë¸ ê¶Œì¥)\n",
        "    # ê¸°ì¡´ pt íŒŒì¼ì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ë¡œë“œí•´ì„œ Fine-tuning, ì—†ë‹¤ë©´ yolov8m.pt ë‹¤ìš´ë¡œë“œ\n",
        "    model = YOLO('yolov8x.pt')\n",
        "\n",
        "    print(\"ğŸ”¥ í•™ìŠµ ì‹œì‘ (Damage Detection)...\")\n",
        "\n",
        "    # 2. í•™ìŠµ ì‹¤í–‰\n",
        "    model.train(\n",
        "        data=YAML_PATH,\n",
        "        epochs=100,            # 50 Epoch (ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì ˆ)\n",
        "        imgsz=1024,            # Cropëœ ì´ë¯¸ì§€ì´ë¯€ë¡œ 640ì´ë©´ ì¶©ë¶„\n",
        "        batch=16,\n",
        "        patience=10,          # 10ë²ˆ ë™ì•ˆ ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ\n",
        "        project=SAVE_DIR,     # ì €ì¥í•  í”„ë¡œì íŠ¸ í´ë”\n",
        "        name='yolov8x_damage_class_all_2nd',# ì €ì¥í•  í•˜ìœ„ í´ë” ì´ë¦„\n",
        "        exist_ok=True,\n",
        "        optimizer='AdamW',\n",
        "        lr0=1e-4,             # Fine-tuningì´ë¯€ë¡œ í•™ìŠµë¥ ì„ ì¡°ê¸ˆ ë‚®ê²Œ ì„¤ì •\n",
        "        close_mosaic=20,      # ë§ˆì§€ë§‰ 10 epochì€ Mosaic Augmentation ë„ê¸° (ì •ë°€ë„ í–¥ìƒ)\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    return os.path.join(SAVE_DIR, 'yolov8m_damage', 'weights', 'best.pt')\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "best_model_path = train_model()\n",
        "print(f\"ğŸ† Best Model Path: {best_model_path}\")"
      ],
      "metadata": {
        "id": "SGFtOdaQ6t8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(test)"
      ],
      "metadata": {
        "id": "pTJNrO426t8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/TRAIN_RESULT/yolov8x_damage_class_all_2nd/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_damage_test_results_2nd_025.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ],
      "metadata": {
        "id": "P-nBwvbg6t8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ],
      "metadata": {
        "id": "a7kGER4M6t8U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPiEqNBE6t8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8x_damage_class_all_2nd', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_2nd_025.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=10)"
      ],
      "metadata": {
        "id": "mIps6gu86t8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25->0.1)"
      ],
      "metadata": {
        "id": "SjxULrzG6t8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/TRAIN_RESULT/yolov8x_damage_class_all_2nd/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.1, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_damage_test_results_2nd_010.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ],
      "metadata": {
        "id": "-vA-pyrP6t8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8x_damage_class_all_2nd', 'weights', 'best.pt')\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_damage_test_results_2nd_010.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=6)"
      ],
      "metadata": {
        "id": "jEgH8Ryx6t8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9eKgj-f6t8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}