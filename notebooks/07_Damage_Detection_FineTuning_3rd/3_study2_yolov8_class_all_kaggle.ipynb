{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n81wrpUcfmvk"
      },
      "source": [
        "# study2 ëª¨ë¸. ì°¨ëŸ‰ íŒŒì† ì—¬ë¶€ íƒì§€ + ver2.0(kaggle data ì¶”ê°€) + ver3.0(ai-hub 12000ì¥)\n",
        "\n",
        " ## 1. ë°ì´í„°ì…‹ êµ¬ì¶• ì›Œí¬í”Œë¡œìš° (Workflow)\n",
        " #### 1) ì¤€ë¹„ (Preparation)\n",
        "  - ì…ë ¥ ë°ì´í„°:\n",
        "    - Damaged í´ë” (íŒŒì† ì°¨ëŸ‰ ì´ë¯¸ì§€ + íŒŒì† ìœ„ì¹˜ ë¼ë²¨ JSON)\n",
        "    - Normal í´ë” (ì •ìƒ ì°¨ëŸ‰ ì´ë¯¸ì§€)\n",
        " - ë„êµ¬ (Tool):\n",
        "    - Study 1 ëª¨ë¸: ì´ë¯¸ í•™ìŠµëœ ì°¨ëŸ‰ íƒì§€ ëª¨ë¸ (yolov8_fine_tuning.pt)\n",
        " #### 2) ë°ì´í„° ë¶„í•  (Splitting)\n",
        "  - ë¹„ìœ¨: ì „ì²´ ë°ì´í„°ë¥¼ Train (70%) : Val (20%) : Test (10%) ë¹„ìœ¨ë¡œ ë¬´ì‘ìœ„ ë¶„í• \n",
        "  - ëª©ì : í•™ìŠµìš©, ê²€ì¦ìš©, í‰ê°€ìš© ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë‚˜ëˆ„ì–´ ë°ì´í„° ìœ ì¶œ(Data Leakage)ì„ ë°©ì§€\n",
        " #### 3) í•µì‹¬ ì²˜ë¦¬ ê³¼ì • (Core Process) - ë°˜ë³µë¬¸(Loop)\n",
        "  - ì°¨ëŸ‰ íƒì§€ (Detect Vehicle):\n",
        "    - 1ë‹¨ê³„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ **ì°¨ëŸ‰ì˜ìœ„ì¹˜BBox**ë¥¼ ì°¾ìŒ\n",
        "    - (ë§Œì•½ ì°¨ëŸ‰ì„ ëª» ì°¾ìœ¼ë©´? â†’ ì›ë³¸ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì‚¬ìš©í•˜ë„ë¡ Fallback)\n",
        " - ì˜ì—­ ìë¥´ê¸° (Crop with Margin):\n",
        "    - ì°¨ëŸ‰ BBoxì— **ì—¬ìœ ê³µê°„(Margin15%)** ì„ ë”í•´ì„œ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ëƒ„.\n",
        "    - ì´ë•Œ, ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ì¢Œí‘œëŠ” **Clamp(ìš¸íƒ€ë¦¬)** ì²˜ë¦¬í•˜ì—¬ ì´ë¯¸ì§€ ì•ˆìª½ìœ¼ë¡œ ë§ì¶¤\n",
        " - í¬ê¸° ì¡°ì • (Resize):\n",
        "    - ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ í¬ê¸°(ì˜ˆ: 640x640 ë˜ëŠ” 1024x1024)ë¡œ ë³€ê²½\n",
        " - ë¼ë²¨ ë³€í™˜ (Label Remapping):\n",
        "    - Damaged: ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ì˜ íŒŒì† ì¢Œí‘œ (x, y)ë¥¼ Cropëœ ì´ë¯¸ì§€ ê¸°ì¤€ì˜ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜.(ì´ë•Œë„ ì¢Œí‘œê°€ íŠ€ì§€ ì•Šê²Œ Clamp ì ìš© í•„ìˆ˜)\n",
        "    - Normal: íŒŒì†ì´ ì—†ìœ¼ë¯€ë¡œ **ë¹ˆ í…ìŠ¤íŠ¸ íŒŒì¼.txt**ì„ ìƒì„± (Negative Sample í•™ìŠµìš©)\n",
        " #### 4) ì €ì¥ (Saving)\n",
        " - ë³€í™˜ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨(.txt)ì„ YOLO í•™ìŠµ í¬ë§·ì— ë§ì¶° ì €ì¥\n",
        "    - images/train,\n",
        "    - labels/trainimages/val,\n",
        "    - labels/valimages/test, labels/test\n",
        " - **í´ë˜ìŠ¤ í†µí•©**: ë°ì´í„° ë³µì¡ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë“  íŒŒì† ì¢…ë¥˜(Scratch, Dent ë“±)ë¥¼ 'Damage' (ID: 0) í•˜ë‚˜ë¡œ í†µí•©\n",
        " #### 5) ê²€ì¦ (Verification)\n",
        " - ì‹œê°í™”: ë³€í™˜ëœ ë°ì´í„°(Test Set) ì¤‘ 3ì¥ì„ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„, íŒŒì† ë°•ìŠ¤ê°€ ì°¨ëŸ‰ ìœ„ì— ì •í™•íˆ ê·¸ë ¤ì¡ŒëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### trainì‹œ 1ì‹œê°„ ì •ë„ ì†Œëª¨ ë©ë‹ˆë‹¤. colabí™˜ê²½ì—ì„œ ì•„ë˜ë¥¼ browserì˜ console ì—ì„œ ë¶™ì—¬ ë„£ê¸°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "### shift+cntr+i ë¡œ browser console ì—´ê¸°\n",
        "# https://github.com/chulminkw/DLCV/blob/master/data/util/colab_autoclick.js\n",
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "metadata": {
        "id": "XSd9IvVGp2sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2-e3-P3Of7a"
      },
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOGE-DmT1CjC"
      },
      "outputs": [],
      "source": [
        "# [Cell 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# YOLOv8 ë° ë°ì´í„° ì²˜ë¦¬ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install ultralytics tqdm\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. ì••ì¶•ì„ í’€ í´ë” ìƒì„± (íŒŒì¼ì´ ì„ì´ì§€ ì•Šê²Œ í´ë”ë¥¼ ë”°ë¡œ ë§Œë“œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)\n",
        "extract_path = \"/content/dataset\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 2. ì••ì¶• í•´ì œ ì‹¤í–‰\n",
        "# -q: Quiet ëª¨ë“œ (ë¡œê·¸ ìˆ¨ê¹€ -> ì†ë„ í–¥ìƒ)\n",
        "# -d: Destination (ì €ì¥í•  ê²½ë¡œ ì§€ì •)\n",
        "!unzip -q /content/AI_HUB_DAMAGE_DATASET.zip -d {extract_path}\n",
        "!unzip -q /content/RAW.zip -d {extract_path}\n",
        "\n",
        "print(f\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ! ê²½ë¡œ: {extract_path}\")"
      ],
      "metadata": {
        "id": "DR9NuWBmvBP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„°ì…‹ êµ¬ì¶• ë° ë¶„í• "
      ],
      "metadata": {
        "id": "7BKdx0kevnsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# =========================================================\n",
        "# âš™ï¸ [ì„¤ì •] ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° (ìˆ˜ì •ë¨)\n",
        "# =========================================================\n",
        "# 1. ì›ë³¸ ë°ì´í„° ê²½ë¡œ (ë³€ê²½ëœ ë¶€ë¶„)\n",
        "# 1. ì…ë ¥ ë°ì´í„° ê²½ë¡œ (Source)\n",
        "# (1) AI-Hub (Damaged)\n",
        "#AI_HUB_IMG_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/AI_HUB_DAMAGE_DATASET/images\" # ì†ë„ ê°œì„ ì„ ìœ„í•´ contentì— ì§ì ‘ ì—…ë¡œë“œ\n",
        "#AI_HUB_LBL_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/AI_HUB_DAMAGE_DATASET/labels\"\n",
        "AI_HUB_IMG_ROOT = \"/content/dataset/AI_HUB_DAMAGE_DATASET/images\" # ì†ë„ ê°œì„ ì„ ìœ„í•´ contentì— ì§ì ‘ ì—…ë¡œë“œ\n",
        "AI_HUB_LBL_ROOT = \"/content/dataset/AI_HUB_DAMAGE_DATASET/labels\"\n",
        "\n",
        "# (2) Normal & Background (Source Base)\n",
        "#NORMAL_ROOT_BASE = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/RAW\"\n",
        "NORMAL_ROOT_BASE = \"/content/dataset/RAW\"\n",
        "\n",
        "\n",
        "# 2. ì €ì¥í•  ë°ì´í„°ì…‹ ê²½ë¡œ\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "\n",
        "# 3. 1ë‹¨ê³„ ì°¨ëŸ‰ íƒì§€ ëª¨ë¸ ê²½ë¡œ (â˜… ì¤‘ìš”: ì‹¤ì œ pt íŒŒì¼ ê²½ë¡œê°€ ë§ëŠ”ì§€ í™•ì¸ í•„ìš”)\n",
        "MODEL_PATH = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK1_CAR_DETECTION/FINE_TUNING_MODEL/yolov8x_fine_tuning_5th/weights/best.pt\"\n",
        "\n",
        "# 4. íŒŒë¼ë¯¸í„°\n",
        "IMG_SIZE = 640       # Stage 2 ì…ë ¥ ì‚¬ì´ì¦ˆ\n",
        "MARGIN_RATIO = 0.15  # Crop ë§ˆì§„\n",
        "CONF_THRES = 0.1     # [ìš”ì²­ë°˜ì˜] ì°¨ëŸ‰ íƒì§€ ì„ê³„ê°’ (Recall 100% ê¸°ì¤€)\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"âœ… ì„¤ì • ë° ê²½ë¡œ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "TmJTnJYKvoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ (Clamp, ì¢Œí‘œ ë³€í™˜ - ê¸°ì¡´ ìœ ì§€)\n",
        "\n",
        "\n",
        "def clamp(value, min_val, max_val):\n",
        "    return max(min_val, min(value, max_val))\n",
        "\n",
        "def get_vehicle_crop_box(image, model, margin=0.15):\n",
        "    \"\"\"Stage 1 ëª¨ë¸ë¡œ ì°¨ëŸ‰ì„ ì°¾ì•„ Crop ì¢Œí‘œ ë°˜í™˜ (conf=0.1 ì ìš©)\"\"\"\n",
        "    h_img, w_img = image.shape[:2]\n",
        "\n",
        "    # 1. ì¶”ë¡ \n",
        "    results = model(image, verbose=False, conf=CONF_THRES)\n",
        "\n",
        "    best_box = None\n",
        "    max_area = 0\n",
        "\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "            coords = box.xyxy[0].cpu().numpy() # x1, y1, x2, y2\n",
        "            area = (coords[2] - coords[0]) * (coords[3] - coords[1])\n",
        "            if area > max_area:\n",
        "                max_area = area\n",
        "                best_box = coords\n",
        "\n",
        "    # 2. Fallback (ë¯¸íƒì§€ ì‹œ ì „ì²´ ì´ë¯¸ì§€ ì‚¬ìš© - ê·¼ì ‘ìƒ· ëŒ€ì‘)\n",
        "    if best_box is None:\n",
        "        return 0, 0, w_img, h_img, False\n",
        "\n",
        "    # 3. Margin ì ìš© & Clamp\n",
        "    x1, y1, x2, y2 = best_box\n",
        "    w_box = x2 - x1; h_box = y2 - y1\n",
        "\n",
        "    nx1 = int(clamp(x1 - w_box * margin, 0, w_img))\n",
        "    ny1 = int(clamp(y1 - h_box * margin, 0, h_img))\n",
        "    nx2 = int(clamp(x2 + w_box * margin, 0, w_img))\n",
        "    ny2 = int(clamp(y2 + h_box * margin, 0, h_img))\n",
        "\n",
        "    return nx1, ny1, nx2, ny2, True\n",
        "\n",
        "def convert_label_global_to_local(src_txt_path, crop_coords, src_size):\n",
        "    \"\"\"\n",
        "    [í•µì‹¬] ì›ë³¸ TXT ë¼ë²¨ì„ ì½ì–´ Cropëœ ì´ë¯¸ì§€ ê¸°ì¤€ ì¢Œí‘œë¡œ ë³€í™˜\n",
        "    src_txt_path: ì›ë³¸ ë¼ë²¨ ê²½ë¡œ\n",
        "    crop_coords: (cx1, cy1, cx2, cy2) - í¬ë¡­ ì˜ì—­\n",
        "    src_size: (w_org, h_org) - ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°\n",
        "    \"\"\"\n",
        "    if not os.path.exists(src_txt_path):\n",
        "        return []\n",
        "\n",
        "    cx1, cy1, cx2, cy2 = crop_coords\n",
        "    w_org, h_org = src_size\n",
        "    crop_w = cx2 - cx1\n",
        "    crop_h = cy2 - cy1\n",
        "\n",
        "    new_labels = []\n",
        "\n",
        "    with open(src_txt_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 5: continue\n",
        "\n",
        "        cls = int(parts[0]) # í´ë˜ìŠ¤ ID (ì—¬ê¸°ì„œëŠ” ëª¨ë‘ 0ìœ¼ë¡œ í†µí•© ì˜ˆì •ì´ë©´ ë‚˜ì¤‘ì— ì²˜ë¦¬)\n",
        "        # ì›ë³¸ ì •ê·œí™” ì¢Œí‘œ\n",
        "        n_cx, n_cy, n_w, n_h = map(float, parts[1:5])\n",
        "\n",
        "        # 1. Denormalize (í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜)\n",
        "        gx_center = n_cx * w_org\n",
        "        gy_center = n_cy * h_org\n",
        "        gw = n_w * w_org\n",
        "        gh = n_h * h_org\n",
        "\n",
        "        gx1 = gx_center - gw / 2\n",
        "        gy1 = gy_center - gh / 2\n",
        "        gx2 = gx_center + gw / 2\n",
        "        gy2 = gy_center + gh / 2\n",
        "\n",
        "        # 2. Intersection (Crop ì˜ì—­ê³¼ êµì°¨í•˜ëŠ” ë¶€ë¶„ ê³„ì‚°)\n",
        "        # ë¼ë²¨ ë°•ìŠ¤ê°€ í¬ë¡­ ì˜ì—­ ë°–ìœ¼ë¡œ ë‚˜ê°”ëŠ”ì§€ í™•ì¸\n",
        "        ix1 = max(gx1, cx1)\n",
        "        iy1 = max(gy1, cy1)\n",
        "        ix2 = min(gx2, cx2)\n",
        "        iy2 = min(gy2, cy2)\n",
        "\n",
        "        # êµì°¨ ì˜ì—­ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´(ì˜ë ¤ ë‚˜ê°) ìŠ¤í‚µ\n",
        "        if ix2 <= ix1 or iy2 <= iy1:\n",
        "            continue\n",
        "\n",
        "        # 3. Remapping (Crop ì˜ì—­ ê¸°ì¤€ ì¢Œí‘œë¡œ ë³€í™˜)\n",
        "        lx1 = ix1 - cx1\n",
        "        ly1 = iy1 - cy1\n",
        "        lx2 = ix2 - cx1\n",
        "        ly2 = iy2 - cy1\n",
        "\n",
        "        lw = lx2 - lx1\n",
        "        lh = ly2 - ly1\n",
        "\n",
        "        # ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ëŠ” ì œê±° (ë…¸ì´ì¦ˆ ë°©ì§€)\n",
        "        if lw < 2 or lh < 2: continue\n",
        "\n",
        "        # 4. Normalize (Crop ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)\n",
        "        final_cx = (lx1 + lw / 2) / crop_w\n",
        "        final_cy = (ly1 + lh / 2) / crop_h\n",
        "        final_w = lw / crop_w\n",
        "        final_h = lh / crop_h\n",
        "\n",
        "        # ê°’ í´ë¨í•‘ (0~1 ì‚¬ì´ ì•ˆì „ì¥ì¹˜)\n",
        "        final_cx = clamp(final_cx, 0, 1)\n",
        "        final_cy = clamp(final_cy, 0, 1)\n",
        "        final_w = clamp(final_w, 0, 1)\n",
        "        final_h = clamp(final_h, 0, 1)\n",
        "\n",
        "        # í´ë˜ìŠ¤ IDëŠ” 0 (Damage)ìœ¼ë¡œ í†µí•©\n",
        "        new_labels.append(f\"0 {final_cx:.6f} {final_cy:.6f} {final_w:.6f} {final_h:.6f}\")\n",
        "\n",
        "    return new_labels\n",
        "\n",
        "def process_and_save(src_img_path, src_lbl_path, dst_img_path, dst_txt_path, model):\n",
        "    img = cv2.imread(src_img_path)\n",
        "    if img is None: return False\n",
        "\n",
        "    h_org, w_org = img.shape[:2]\n",
        "\n",
        "    # 1. Crop ì¢Œí‘œ ê³„ì‚°\n",
        "    cx1, cy1, cx2, cy2, detected = get_vehicle_crop_box(img, model, MARGIN_RATIO)\n",
        "    crop_w = cx2 - cx1\n",
        "    crop_h = cy2 - cy1\n",
        "\n",
        "    if crop_w <= 0 or crop_h <= 0: return False\n",
        "\n",
        "    # 2. ì´ë¯¸ì§€ Crop & Resize\n",
        "    crop_img = img[cy1:cy2, cx1:cx2]\n",
        "    resized_img = cv2.resize(crop_img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # 3. ë¼ë²¨ ë³€í™˜ (Damagedì¸ ê²½ìš°ë§Œ)\n",
        "    final_labels = []\n",
        "    if src_lbl_path: # Damaged (Normalì€ None)\n",
        "        final_labels = convert_label_global_to_local(\n",
        "            src_lbl_path, (cx1, cy1, cx2, cy2), (w_org, h_org)\n",
        "        )\n",
        "\n",
        "    # 4. ì €ì¥\n",
        "    cv2.imwrite(dst_img_path, resized_img)\n",
        "    with open(dst_txt_path, 'w') as f:\n",
        "        if final_labels:\n",
        "            f.write(\"\\n\".join(final_labels))\n",
        "\n",
        "    return True\n",
        "\n",
        "print(\"âœ… í•¨ìˆ˜ ë° ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "QQ85MyuYwxek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] ë°ì´í„°ì…‹ êµ¬ì¶• ë©”ì¸ í”„ë¡œì„¸ìŠ¤\n",
        "\n",
        "def build_dataset_stage2():\n",
        "    # 0. ì´ˆê¸°í™”\n",
        "    if os.path.exists(DATASET_ROOT):\n",
        "        shutil.rmtree(DATASET_ROOT)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(DATASET_ROOT, 'images', split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(DATASET_ROOT, 'labels', split), exist_ok=True)\n",
        "\n",
        "    # 1. ëª¨ë¸ ë¡œë“œ\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"âŒ ëª¨ë¸ ì—†ìŒ: {MODEL_PATH}\")\n",
        "        return\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 2. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
        "    print(\"ğŸ“‚ íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\")\n",
        "    extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "\n",
        "    # (1) Damaged Data\n",
        "    d_files = []\n",
        "    for ext in extensions:\n",
        "        d_files.extend(glob.glob(os.path.join(AI_HUB_IMG_ROOT, \"**\", ext), recursive=True))\n",
        "    d_files = list(set(d_files))\n",
        "\n",
        "    # (2) Normal Data (Background ì œì™¸)\n",
        "    n_files = []\n",
        "    target_folders = ['normal', 'normal(kaggle_dataset)']\n",
        "    for folder in target_folders:\n",
        "        path = os.path.join(NORMAL_ROOT_BASE, folder)\n",
        "        if os.path.exists(path):\n",
        "            for ext in extensions:\n",
        "                n_files.extend(glob.glob(os.path.join(path, \"**\", ext), recursive=True))\n",
        "    n_files = list(set(n_files))\n",
        "\n",
        "    print(f\"   - Damaged: {len(d_files)}ì¥\")\n",
        "    print(f\"   - Normal : {len(n_files)}ì¥\")\n",
        "\n",
        "    # 3. ë°ì´í„° ë¶„í• \n",
        "    def split_list(files):\n",
        "        if not files: return [], [], []\n",
        "        train, test = train_test_split(files, test_size=0.1, random_state=SEED)\n",
        "        train, val = train_test_split(train, test_size=0.2222, random_state=SEED)\n",
        "        return train, val, test\n",
        "\n",
        "    dt, dv, dts = split_list(d_files)\n",
        "    nt, nv, nts = split_list(n_files)\n",
        "\n",
        "    # 4. ì²˜ë¦¬ ë£¨í”„\n",
        "    tasks = [\n",
        "        ('train', dt, 'damaged'), ('val', dv, 'damaged'), ('test', dts, 'damaged'),\n",
        "        ('train', nt, 'normal'),  ('val', nv, 'normal'),  ('test', nts, 'normal')\n",
        "    ]\n",
        "\n",
        "    total_stats = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "    print(\"âš¡ Crop ë° ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...\")\n",
        "\n",
        "    for split, files, kind in tasks:\n",
        "        for file_path in tqdm(files, desc=f\"[{split}] {kind}\"):\n",
        "            filename = os.path.basename(file_path)\n",
        "            basename = os.path.splitext(filename)[0]\n",
        "\n",
        "            dst_img = os.path.join(DATASET_ROOT, 'images', split, filename)\n",
        "            dst_txt = os.path.join(DATASET_ROOT, 'labels', split, basename + \".txt\")\n",
        "\n",
        "            src_lbl = None\n",
        "            if kind == 'damaged':\n",
        "                # TXT ë¼ë²¨ ì°¾ê¸° (ì¬ê·€ ê²€ìƒ‰ì€ ëŠë¦¬ë¯€ë¡œ ê²½ë¡œ ë§¤í•‘ ì‹œë„)\n",
        "                # 1. AI_HUB_LBL_ROOT + ìƒëŒ€ê²½ë¡œ (ê°€ì¥ ë¹ ë¦„)\n",
        "                try:\n",
        "                    rel_path = os.path.relpath(os.path.dirname(file_path), AI_HUB_IMG_ROOT)\n",
        "                    candidate = os.path.join(AI_HUB_LBL_ROOT, rel_path, basename + \".txt\")\n",
        "                    if os.path.exists(candidate):\n",
        "                        src_lbl = candidate\n",
        "                except: pass\n",
        "\n",
        "                # 2. ì—†ìœ¼ë©´ LBL_ROOT ë°”ë¡œ ì•„ë˜ í™•ì¸\n",
        "                if src_lbl is None:\n",
        "                    candidate = os.path.join(AI_HUB_LBL_ROOT, basename + \".txt\")\n",
        "                    if os.path.exists(candidate):\n",
        "                        src_lbl = candidate\n",
        "\n",
        "                # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ë¼ë²¨ ì—†ëŠ” ì†ìƒ ë°ì´í„°ëŠ” ì˜ë¯¸ ì—†ìŒ)\n",
        "                if src_lbl is None:\n",
        "                    continue\n",
        "\n",
        "            # ì²˜ë¦¬ ì‹¤í–‰\n",
        "            if process_and_save(file_path, src_lbl, dst_img, dst_txt, model):\n",
        "                total_stats[split] += 1\n",
        "\n",
        "    # 5. YAML ìƒì„±\n",
        "    yaml_txt = f\"\"\"\n",
        "path: {DATASET_ROOT}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "nc: 1\n",
        "names: ['Damage']\n",
        "\"\"\"\n",
        "    with open(os.path.join(DATASET_ROOT, \"data_damage_v2.yaml\"), \"w\") as f:\n",
        "        f.write(yaml_txt)\n",
        "\n",
        "    print(\"\\nâœ… ì™„ë£Œ!\")\n",
        "    print(f\"   Total Generated: {sum(total_stats.values())} files\")\n",
        "    print(f\"   YAML Path: {os.path.join(DATASET_ROOT, 'data_damage_v2.yaml')}\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "build_dataset_stage2()"
      ],
      "metadata": {
        "id": "mJ9G5NOAw8vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì‹œê°í™” (ê²€ì¦)"
      ],
      "metadata": {
        "id": "YSBIdJKd1DCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT"
      ],
      "metadata": {
        "id": "MGa60nGk2h9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 4] ì‹œê°í™” (ê²€ì¦)\n",
        "def visualize_random_samples(num_samples=3):\n",
        "    \"\"\"\n",
        "    ìƒì„±ëœ ë°ì´í„°ì…‹(Test Set) ì¤‘ Damaged ë¼ë²¨ì´ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ ê³¨ë¼ ì‹œê°í™”\n",
        "    \"\"\"\n",
        "    test_img_dir = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    test_lbl_dir = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    # 1. ë¼ë²¨ íŒŒì¼ì´ ìˆëŠ”(ë‚´ìš©ì´ ìˆëŠ”) íŒŒì¼ ì°¾ê¸°\n",
        "    candidates = []\n",
        "    if not os.path.exists(test_lbl_dir):\n",
        "        print(\"í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    for txt_file in os.listdir(test_lbl_dir):\n",
        "        txt_path = os.path.join(test_lbl_dir, txt_file)\n",
        "        if os.path.getsize(txt_path) > 0: # ë‚´ìš©ì´ ìˆìœ¼ë©´ Damageê°€ ìˆëŠ” ê²ƒ\n",
        "            candidates.append(txt_file)\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"ì‹œê°í™”í•  íŒŒì† ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # 2. ëœë¤ ìƒ˜í”Œë§\n",
        "    samples = random.sample(candidates, min(len(candidates), num_samples))\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, txt_name in enumerate(samples):\n",
        "        basename = os.path.splitext(txt_name)[0]\n",
        "        img_name = basename + \".jpg\" # í™•ì¥ìëŠ” jpgë¼ê³  ê°€ì •\n",
        "        if not os.path.exists(os.path.join(test_img_dir, img_name)):\n",
        "            img_name = basename + \".png\"\n",
        "\n",
        "        img_path = os.path.join(test_img_dir, img_name)\n",
        "        txt_path = os.path.join(test_lbl_dir, txt_name)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
        "        with open(txt_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    cls, cx, cy, bw, bh = map(float, parts[:5])\n",
        "\n",
        "                    # YOLO -> Pixel\n",
        "                    x1 = int((cx - bw/2) * w)\n",
        "                    y1 = int((cy - bh/2) * h)\n",
        "                    x2 = int((cx + bw/2) * w)\n",
        "                    y2 = int((cy + bh/2) * h)\n",
        "\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(img, \"Damage\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Sample {i+1}\\n{basename}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ì‹¤í–‰\n",
        "print(\"ğŸ” ìƒì„±ëœ ë°ì´í„°ì…‹ ê²€ì¦ (Random 3 Samples)\")\n",
        "visualize_random_samples()"
      ],
      "metadata": {
        "id": "zmicW6zIxKOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ë¶„ì„í•  ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ\n",
        "# =========================================================\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "\n",
        "def analyze_dataset_status(root_dir):\n",
        "    print(f\"ğŸ“Š ë°ì´í„°ì…‹ í˜„í™© ë¶„ì„ ë³´ê³ ì„œ\")\n",
        "    print(f\"ğŸ“‚ íƒ€ê²Ÿ ê²½ë¡œ: {root_dir}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not os.path.exists(root_dir):\n",
        "        print(\"âŒ ì˜¤ë¥˜: í•´ë‹¹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    splits = ['train', 'val', 'test']\n",
        "    total_summary = {'images': 0, 'labels': 0, 'damaged': 0, 'normal': 0}\n",
        "\n",
        "    # í—¤ë” ì¶œë ¥\n",
        "    print(f\"{'Split':<10} | {'Images':<10} | {'Labels':<10} | {'Damaged(Box)':<12} | {'Normal(Empty)':<12} | {'Status'}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for split in splits:\n",
        "        img_dir = os.path.join(root_dir, 'images', split)\n",
        "        lbl_dir = os.path.join(root_dir, 'labels', split)\n",
        "\n",
        "        # 1. ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸\n",
        "        if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):\n",
        "            print(f\"{split:<10} | {'MISSING DIR':<30} | âŒ Check Path\")\n",
        "            continue\n",
        "\n",
        "        # 2. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—…\n",
        "        # ì´ë¯¸ì§€ í™•ì¥ì ëŒ€ì†Œë¬¸ì ëª¨ë‘ í¬í•¨\n",
        "        img_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "        img_files = []\n",
        "        for ext in img_extensions:\n",
        "            img_files.extend(glob.glob(os.path.join(img_dir, ext)))\n",
        "\n",
        "        lbl_files = glob.glob(os.path.join(lbl_dir, \"*.txt\"))\n",
        "\n",
        "        # 3. ìƒì„¸ ë¶„ì„ (Damaged vs Normal)\n",
        "        # Damaged: txt íŒŒì¼ ì•ˆì— ë‚´ìš©(ì¢Œí‘œ)ì´ ìˆìŒ\n",
        "        # Normal: txt íŒŒì¼ì´ ì—†ê±°ë‚˜, ë¹„ì–´ ìˆìŒ\n",
        "\n",
        "        n_damaged = 0 # ë°•ìŠ¤ê°€ ìˆëŠ” íŒŒì¼\n",
        "        n_normal = 0  # ë¹ˆ íŒŒì¼ (ë˜ëŠ” ë¼ë²¨ íŒŒì¼ ì—†ìŒ)\n",
        "\n",
        "        # íŒŒì¼ëª…(stem) ê¸°ì¤€ ë§¤í•‘ì„ ìœ„í•´ set ìƒì„±\n",
        "        img_stems = {Path(f).stem for f in img_files}\n",
        "        lbl_stems = {Path(f).stem for f in lbl_files}\n",
        "\n",
        "        # ë¼ë²¨ íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ ë¶„ë¥˜\n",
        "        for lbl_path in lbl_files:\n",
        "            if os.path.getsize(lbl_path) > 0:\n",
        "                n_damaged += 1\n",
        "            else:\n",
        "                n_normal += 1\n",
        "\n",
        "        # ë¼ë²¨ íŒŒì¼ ìì²´ê°€ ì—†ëŠ” ê²½ìš°ë„ Normalë¡œ ê°„ì£¼í•  ê²ƒì¸ì§€ ì²´í¬\n",
        "        # (í˜„ì¬ êµ¬ì¶• ë¡œì§ìƒ Normalë„ ë¹ˆ txtë¥¼ ìƒì„±í•˜ë¯€ë¡œ, ë¼ë²¨ íŒŒì¼ ìˆ˜ == ì´ë¯¸ì§€ ìˆ˜ê°€ ì´ìƒì )\n",
        "        missing_labels = len(img_stems - lbl_stems)\n",
        "        if missing_labels > 0:\n",
        "             # ë¼ë²¨ íŒŒì¼ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° (ë³´í†µì€ Normalë¡œ ì·¨ê¸‰í•˜ê±°ë‚˜ ì˜¤ë¥˜)\n",
        "             # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ê°œìˆ˜ë¡œë§Œ íŒŒì•…\n",
        "             pass\n",
        "\n",
        "        # í†µê³„ ì§‘ê³„\n",
        "        n_imgs = len(img_files)\n",
        "        n_lbls = len(lbl_files)\n",
        "\n",
        "        total_summary['images'] += n_imgs\n",
        "        total_summary['labels'] += n_lbls\n",
        "        total_summary['damaged'] += n_damaged\n",
        "        total_summary['normal'] += n_normal + missing_labels # ë¼ë²¨ ì—†ëŠ” ê²ƒë„ Normal ë²”ì£¼ë¡œ ì¼ë‹¨ í¬í•¨\n",
        "\n",
        "        # ìƒíƒœ íŒì •\n",
        "        if n_imgs == n_lbls:\n",
        "            status = \"âœ… Perfect\"\n",
        "        elif n_imgs > n_lbls:\n",
        "            status = f\"âš ï¸ Miss {n_imgs - n_lbls} lbls\"\n",
        "        else:\n",
        "            status = f\"â“ Extra {n_lbls - n_imgs} lbls\"\n",
        "\n",
        "        # ì¶œë ¥\n",
        "        print(f\"{split:<10} | {n_imgs:<10} | {n_lbls:<10} | {n_damaged:<12} | {n_normal:<12} | {status}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"ğŸ“Œ [ì¢…í•© ìš”ì•½]\")\n",
        "    print(f\"   - ì´ ì´ë¯¸ì§€ ìˆ˜ : {total_summary['images']} ì¥\")\n",
        "    print(f\"   - ì´ ë¼ë²¨ íŒŒì¼ : {total_summary['labels']} ê°œ\")\n",
        "    print(f\"   --------------------------------\")\n",
        "    print(f\"   ğŸš— ì†ìƒ ì°¨ëŸ‰ (Positive) : {total_summary['damaged']} ì¥\")\n",
        "    print(f\"   ğŸŸ© ì •ìƒ/ë°°ê²½ (Negative) : {total_summary['normal']} ì¥\")\n",
        "\n",
        "    # YAML ì¡´ì¬ í™•ì¸\n",
        "    yaml_path = os.path.join(root_dir, \"data_damage_v2.yaml\")\n",
        "    if not os.path.exists(yaml_path):\n",
        "        # ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ê²€ìƒ‰\n",
        "        yamls = glob.glob(os.path.join(root_dir, \"*.yaml\"))\n",
        "        if yamls:\n",
        "            print(f\"   ğŸ“„ YAML íŒŒì¼ : âœ… ë°œê²¬ë¨ ({os.path.basename(yamls[0])})\")\n",
        "        else:\n",
        "            print(f\"   ğŸ“„ YAML íŒŒì¼ : âŒ ì—†ìŒ (í™•ì¸ í•„ìš”)\")\n",
        "    else:\n",
        "        print(f\"   ğŸ“„ YAML íŒŒì¼ : âœ… ë°œê²¬ë¨ (data_damage_v2.yaml)\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "analyze_dataset_status(DATASET_ROOT)"
      ],
      "metadata": {
        "id": "K5famByVMOBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ 1ì°¨"
      ],
      "metadata": {
        "id": "P9tF7L_42El-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ\n",
        "# =========================================================\n",
        "# ì›ë³¸ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ)\n",
        "SOURCE_DIR = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "# ëª©ì ì§€ (Colab ë¡œì»¬)\n",
        "DEST_DIR = \"/content/temp\"\n",
        "\n",
        "def copy_file(src_path, dst_path):\n",
        "    shutil.copy2(src_path, dst_path)\n",
        "\n",
        "def fast_copy_with_progress(src, dst):\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"âŒ ì›ë³¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {src}\")\n",
        "        return\n",
        "\n",
        "    print(\"ğŸ” íŒŒì¼ ëª©ë¡ì„ ìŠ¤ìº” ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
        "\n",
        "    # 1. ëª¨ë“  íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—…\n",
        "    files_to_copy = []\n",
        "    for root, dirs, files in os.walk(src):\n",
        "        # ëª©ì ì§€ í´ë” êµ¬ì¡° ë¯¸ë¦¬ ìƒì„±\n",
        "        rel_path = os.path.relpath(root, src)\n",
        "        dest_path = os.path.join(dst, rel_path)\n",
        "        os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "        for f in files:\n",
        "            src_file = os.path.join(root, f)\n",
        "            dst_file = os.path.join(dest_path, f)\n",
        "            files_to_copy.append((src_file, dst_file))\n",
        "\n",
        "    total_files = len(files_to_copy)\n",
        "    print(f\"ğŸ“¦ ì´ ë³µì‚¬í•  íŒŒì¼ ìˆ˜: {total_files}ê°œ\")\n",
        "    print(f\"ğŸš€ ë©€í‹°ì“°ë ˆë”© ë³µì‚¬ ì‹œì‘ (8 workers)...\")\n",
        "\n",
        "    # 2. ë³‘ë ¬ ë³µì‚¬ ì‹¤í–‰ (Thread Pool)\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # tqdmìœ¼ë¡œ ì§„í–‰ë°” ì—°ê²°\n",
        "        list(tqdm(executor.map(lambda x: copy_file(x[0], x[1]), files_to_copy),\n",
        "                  total=total_files,\n",
        "                  unit='file'))\n",
        "\n",
        "    print(\"\\nâœ… ë°ì´í„°ì…‹ ë³µì‚¬ ì™„ë£Œ!\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if os.path.exists(DEST_DIR):\n",
        "    shutil.rmtree(DEST_DIR) # ê¸°ì¡´êº¼ ì‚­ì œ í›„ ì¬ì‹œì‘\n",
        "\n",
        "fast_copy_with_progress(SOURCE_DIR, DEST_DIR)"
      ],
      "metadata": {
        "id": "lo9MS2ATNclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yaml\n",
        "# =========================================================\n",
        "# 2. [ì„¤ì •] YAML íŒŒì¼ ê²½ë¡œ ì¬ì§€ì •\n",
        "# =========================================================\n",
        "# ë¡œì»¬ë¡œ ë³µì‚¬í–ˆìœ¼ë¯€ë¡œ YAML íŒŒì¼ ë‚´ì˜ pathë„ ìˆ˜ì •í•´ì¤˜ì•¼ í•¨\n",
        "yaml_path_local = os.path.join(DEST_DIR, 'data_damage_v2.yaml')\n",
        "\n",
        "# YAML ë‚´ìš© ìˆ˜ì • (path: /content/temp_dataset ìœ¼ë¡œ ë³€ê²½)\n",
        "with open(yaml_path_local, 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "data['path'] = DEST_DIR  # ê²½ë¡œë¥¼ ë¡œì»¬ë¡œ ë³€ê²½\n",
        "\n",
        "with open(yaml_path_local, 'w') as f:\n",
        "    yaml.dump(data, f)\n",
        "\n",
        "print(f\"ğŸ“‚ í•™ìŠµìš© YAML ê²½ë¡œ: {yaml_path_local}\")"
      ],
      "metadata": {
        "id": "xVwDJ313Pdwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì²­ì†Œ\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JshB8DUgWht-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# 3. [ì„¤ì •] ì €ì¥ ê²½ë¡œ ë° ëª¨ë¸ í•™ìŠµ\n",
        "# =========================================================\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "SAVE_DIR = os.path.join(PROJECT_DIR, \"FINE_TUNING_MODEL\")\n",
        "\n",
        "def train_model():\n",
        "    model = YOLO('yolov8m.pt')\n",
        "\n",
        "    print(\"ğŸ”¥ Stage 2 í•™ìŠµ ì‹œì‘...\")\n",
        "    model.train(\n",
        "        data=yaml_path_local, # ë¡œì»¬ YAML ì‚¬ìš©\n",
        "        epochs=100,\n",
        "        imgsz=640,\n",
        "        batch=32,\n",
        "\n",
        "        # [íŠœë‹ í¬ì¸íŠ¸ ë°˜ì˜]\n",
        "        patience=25,       # 10ì€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. 25ë¡œ ëŠ˜ë¦¬ì„¸ìš”.\n",
        "        optimizer='AdamW',\n",
        "        lr0=1e-4,\n",
        "        cos_lr=True,       # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (ê¶Œì¥)\n",
        "        single_cls=True,   # ëª¨ë“  ë¼ë²¨ì„ Damage í•˜ë‚˜ë¡œ í†µí•© (ê¶Œì¥)\n",
        "\n",
        "        close_mosaic=10,   # ì˜¤íƒ€ ìˆ˜ì •ë¨ (OK)\n",
        "\n",
        "        project=SAVE_DIR,\n",
        "        name='yolov8m_damage_stage2_3rd',\n",
        "        exist_ok=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    return os.path.join(SAVE_DIR, 'yolov8m_damage_stage2_3rd', 'weights', 'best.pt')\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "best_model_path = train_model()\n",
        "print(f\"ğŸ† Best Model Path: {best_model_path}\")"
      ],
      "metadata": {
        "id": "H6nXW5MeNuvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypwwQJIXu1eW"
      },
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VenRcE_cu1eZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/FINE_TUNING_MODEL/yolov8m_damage_stage2_3rd/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage_v2.yaml')\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_finetuned_3rd_test_results_conf025.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3uEav7Gu1ea"
      },
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyFjgyIlu1eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)\n",
        "# =========================================================\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "\n",
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_stage2_3rd', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_3rd_test_results_conf025.csv\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë° ë¼ë²¨ ê²½ë¡œ\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "def visualize_failures_batch(start_index=0, batch_size=3):\n",
        "    \"\"\"\n",
        "    ì˜¤íƒ ë°ì´í„°ë¥¼ start_indexë¶€í„° batch_sizeë§Œí¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # 1. ê²°ê³¼ CSV ë¡œë“œ ë° ì˜¤íƒ í•„í„°ë§\n",
        "    if not os.path.exists(RESULT_CSV_PATH):\n",
        "        print(f\"âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RESULT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(RESULT_CSV_PATH)\n",
        "    failures = df[df['is_correct'] == False] # ì˜¤íƒë§Œ ì¶”ì¶œ\n",
        "\n",
        "    total_failures = len(failures)\n",
        "    if total_failures == 0:\n",
        "        print(\"ğŸ‰ ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! ì™„ë²½í•©ë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬\n",
        "    if start_index >= total_failures:\n",
        "        print(f\"âš ï¸ ë” ì´ìƒ í™•ì¸í•  ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_failures}ê±´)\")\n",
        "        return\n",
        "\n",
        "    end_index = min(start_index + batch_size, total_failures)\n",
        "    target_samples = failures.iloc[start_index : end_index]\n",
        "\n",
        "    print(f\"ğŸ” ì˜¤íƒ ë°ì´í„° ì‹œê°í™”: {start_index+1} ~ {end_index} / ì´ {total_failures}ê±´\")\n",
        "\n",
        "    # 2. ëª¨ë¸ ë¡œë“œ (ë°•ìŠ¤ ì‹œê°í™”ìš©)\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 3. ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 6 * len(target_samples)))\n",
        "\n",
        "    for i, (_, row) in enumerate(target_samples.iterrows()):\n",
        "        filename = row['filename']\n",
        "        true_label = row['true_label'] # 0: Normal, 1: Damaged\n",
        "        img_path = os.path.join(TEST_IMG_DIR, filename)\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # A. ì •ë‹µ(Ground Truth) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    x1 = int((cx - bw/2) * w)\n",
        "                    y1 = int((cy - bh/2) * h)\n",
        "                    x2 = int((cx + bw/2) * w)\n",
        "                    y2 = int((cy + bh/2) * h)\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "                    cv2.putText(img, \"GT\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        # B. ëª¨ë¸ ì˜ˆì¸¡(Prediction) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "        # ì‹œê°í™”ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì¶”ë¡  ìˆ˜í–‰\n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                cv2.putText(img, f\"Pred {conf:.2f}\", (x1, y2+25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "        # íƒ€ì´í‹€ ì„¤ì •\n",
        "        str_true = \"Damaged\" if true_label == 1 else \"Normal\"\n",
        "        str_pred = \"Normal\" if true_label == 1 else \"Damaged\" # ì˜¤íƒì´ë¯€ë¡œ ë°˜ëŒ€\n",
        "        fail_type = \"False Negative (ë¯¸íƒ)\" if true_label == 1 else \"False Positive (ì˜¤íƒ/ê³¼íƒ)\"\n",
        "\n",
        "        plt.subplot(len(target_samples), 1, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"[{start_index+i+1}] {filename}\\nType: {fail_type} | GT: {str_true} (Green) vs Pred: {str_pred} (Red)\",\n",
        "                  fontsize=14, color='red', fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM1vPQ74u1eb"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_stage2_3rd', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_3rd_test_results_conf025.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl1aM7RYu1ec"
      },
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25->0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3ZC27VfUhrY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/FINE_TUNING_MODEL/yolov8m_damage_stage2_3rd/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage_v2.yaml')\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.1, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_finetuned_3rd_test_results_conf010.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQkQrLKGUhra"
      },
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD5htGDOUhrb"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8m_damage_stage2_3rd', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_3rd_test_results_conf010.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5acwidgRVRBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ 2ì°¨"
      ],
      "metadata": {
        "id": "hYYZNMyrT01r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ\n",
        "# =========================================================\n",
        "# ì›ë³¸ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ)\n",
        "SOURCE_DIR = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "# ëª©ì ì§€ (Colab ë¡œì»¬)\n",
        "DEST_DIR = \"/content/temp\"\n",
        "\n",
        "def copy_file(src_path, dst_path):\n",
        "    shutil.copy2(src_path, dst_path)\n",
        "\n",
        "def fast_copy_with_progress(src, dst):\n",
        "    if not os.path.exists(src):\n",
        "        print(f\"âŒ ì›ë³¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {src}\")\n",
        "        return\n",
        "\n",
        "    print(\"ğŸ” íŒŒì¼ ëª©ë¡ì„ ìŠ¤ìº” ì¤‘ì…ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
        "\n",
        "    # 1. ëª¨ë“  íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—…\n",
        "    files_to_copy = []\n",
        "    for root, dirs, files in os.walk(src):\n",
        "        # ëª©ì ì§€ í´ë” êµ¬ì¡° ë¯¸ë¦¬ ìƒì„±\n",
        "        rel_path = os.path.relpath(root, src)\n",
        "        dest_path = os.path.join(dst, rel_path)\n",
        "        os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "        for f in files:\n",
        "            src_file = os.path.join(root, f)\n",
        "            dst_file = os.path.join(dest_path, f)\n",
        "            files_to_copy.append((src_file, dst_file))\n",
        "\n",
        "    total_files = len(files_to_copy)\n",
        "    print(f\"ğŸ“¦ ì´ ë³µì‚¬í•  íŒŒì¼ ìˆ˜: {total_files}ê°œ\")\n",
        "    print(f\"ğŸš€ ë©€í‹°ì“°ë ˆë”© ë³µì‚¬ ì‹œì‘ (8 workers)...\")\n",
        "\n",
        "    # 2. ë³‘ë ¬ ë³µì‚¬ ì‹¤í–‰ (Thread Pool)\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # tqdmìœ¼ë¡œ ì§„í–‰ë°” ì—°ê²°\n",
        "        list(tqdm(executor.map(lambda x: copy_file(x[0], x[1]), files_to_copy),\n",
        "                  total=total_files,\n",
        "                  unit='file'))\n",
        "\n",
        "    print(\"\\nâœ… ë°ì´í„°ì…‹ ë³µì‚¬ ì™„ë£Œ!\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if os.path.exists(DEST_DIR):\n",
        "    shutil.rmtree(DEST_DIR) # ê¸°ì¡´êº¼ ì‚­ì œ í›„ ì¬ì‹œì‘\n",
        "\n",
        "fast_copy_with_progress(SOURCE_DIR, DEST_DIR)"
      ],
      "metadata": {
        "id": "KujK9GtpT01s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yaml\n",
        "# =========================================================\n",
        "# 2. [ì„¤ì •] YAML íŒŒì¼ ê²½ë¡œ ì¬ì§€ì •\n",
        "# =========================================================\n",
        "# ë¡œì»¬ë¡œ ë³µì‚¬í–ˆìœ¼ë¯€ë¡œ YAML íŒŒì¼ ë‚´ì˜ pathë„ ìˆ˜ì •í•´ì¤˜ì•¼ í•¨\n",
        "yaml_path_local = os.path.join(DEST_DIR, 'data_damage_v2.yaml')\n",
        "\n",
        "# YAML ë‚´ìš© ìˆ˜ì • (path: /content/temp_dataset ìœ¼ë¡œ ë³€ê²½)\n",
        "with open(yaml_path_local, 'r') as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "data['path'] = DEST_DIR  # ê²½ë¡œë¥¼ ë¡œì»¬ë¡œ ë³€ê²½\n",
        "\n",
        "with open(yaml_path_local, 'w') as f:\n",
        "    yaml.dump(data, f)\n",
        "\n",
        "print(f\"ğŸ“‚ í•™ìŠµìš© YAML ê²½ë¡œ: {yaml_path_local}\")"
      ],
      "metadata": {
        "id": "yW-DuQA8T01u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì²­ì†Œ\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "VbMmkAf5T01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# 3. [ì„¤ì •] ì €ì¥ ê²½ë¡œ ë° ëª¨ë¸ í•™ìŠµ\n",
        "# =========================================================\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "SAVE_DIR = os.path.join(PROJECT_DIR, \"FINE_TUNING_MODEL\")\n",
        "\n",
        "def train_model():\n",
        "    model = YOLO('yolov8x.pt')\n",
        "\n",
        "    print(\"ğŸ”¥ Stage 2 í•™ìŠµ ì‹œì‘...\")\n",
        "    model.train(\n",
        "        data=yaml_path_local, # ë¡œì»¬ YAML ì‚¬ìš©\n",
        "        epochs=100,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "\n",
        "        # [íŠœë‹ í¬ì¸íŠ¸ ë°˜ì˜]\n",
        "        patience=25,       # 10ì€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. 25ë¡œ ëŠ˜ë¦¬ì„¸ìš”.\n",
        "        optimizer='AdamW',\n",
        "        lr0=1e-4,\n",
        "        cos_lr=True,       # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (ê¶Œì¥)\n",
        "        single_cls=True,   # ëª¨ë“  ë¼ë²¨ì„ Damage í•˜ë‚˜ë¡œ í†µí•© (ê¶Œì¥)\n",
        "\n",
        "        close_mosaic=10,   # ì˜¤íƒ€ ìˆ˜ì •ë¨ (OK)\n",
        "\n",
        "        project=SAVE_DIR,\n",
        "        name='yolov8x_damage_stage2_4th',\n",
        "        exist_ok=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    print(\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    return os.path.join(SAVE_DIR, 'yolov8x_damage_stage2_4th', 'weights', 'best.pt')\n",
        "\n",
        "# í•™ìŠµ ì‹¤í–‰\n",
        "best_model_path = train_model()\n",
        "print(f\"ğŸ† Best Model Path: {best_model_path}\")"
      ],
      "metadata": {
        "id": "3BKoGzUhT01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ULcaHtT01w"
      },
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQCezczhT01w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/FINE_TUNING_MODEL/yolov8x_damage_stage2_4th/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage_v2.yaml')\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_finetuned_4th_test_results_conf025.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhjeZoiqT01x"
      },
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TETFnH6sT01y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# =========================================================\n",
        "# [ì„¤ì •] ê²½ë¡œ (ì´ì „ ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)\n",
        "# =========================================================\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "\n",
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8x_damage_stage2_4th', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_4th_test_results_conf025.csv\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë° ë¼ë²¨ ê²½ë¡œ\n",
        "TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "def visualize_failures_batch(start_index=0, batch_size=3):\n",
        "    \"\"\"\n",
        "    ì˜¤íƒ ë°ì´í„°ë¥¼ start_indexë¶€í„° batch_sizeë§Œí¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # 1. ê²°ê³¼ CSV ë¡œë“œ ë° ì˜¤íƒ í•„í„°ë§\n",
        "    if not os.path.exists(RESULT_CSV_PATH):\n",
        "        print(f\"âŒ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RESULT_CSV_PATH}\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(RESULT_CSV_PATH)\n",
        "    failures = df[df['is_correct'] == False] # ì˜¤íƒë§Œ ì¶”ì¶œ\n",
        "\n",
        "    total_failures = len(failures)\n",
        "    if total_failures == 0:\n",
        "        print(\"ğŸ‰ ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! ì™„ë²½í•©ë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬\n",
        "    if start_index >= total_failures:\n",
        "        print(f\"âš ï¸ ë” ì´ìƒ í™•ì¸í•  ì˜¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_failures}ê±´)\")\n",
        "        return\n",
        "\n",
        "    end_index = min(start_index + batch_size, total_failures)\n",
        "    target_samples = failures.iloc[start_index : end_index]\n",
        "\n",
        "    print(f\"ğŸ” ì˜¤íƒ ë°ì´í„° ì‹œê°í™”: {start_index+1} ~ {end_index} / ì´ {total_failures}ê±´\")\n",
        "\n",
        "    # 2. ëª¨ë¸ ë¡œë“œ (ë°•ìŠ¤ ì‹œê°í™”ìš©)\n",
        "    model = YOLO(MODEL_PATH)\n",
        "\n",
        "    # 3. ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 6 * len(target_samples)))\n",
        "\n",
        "    for i, (_, row) in enumerate(target_samples.iterrows()):\n",
        "        filename = row['filename']\n",
        "        true_label = row['true_label'] # 0: Normal, 1: Damaged\n",
        "        img_path = os.path.join(TEST_IMG_DIR, filename)\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # A. ì •ë‹µ(Ground Truth) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    cls, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "                    x1 = int((cx - bw/2) * w)\n",
        "                    y1 = int((cy - bh/2) * h)\n",
        "                    x2 = int((cx + bw/2) * w)\n",
        "                    y2 = int((cy + bh/2) * h)\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "                    cv2.putText(img, \"GT\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        # B. ëª¨ë¸ ì˜ˆì¸¡(Prediction) ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)\n",
        "        # ì‹œê°í™”ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì¶”ë¡  ìˆ˜í–‰\n",
        "        results = model(img_path, conf=0.25, verbose=False)\n",
        "        for r in results:\n",
        "            for box in r.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                conf = float(box.conf[0])\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                cv2.putText(img, f\"Pred {conf:.2f}\", (x1, y2+25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
        "\n",
        "        # íƒ€ì´í‹€ ì„¤ì •\n",
        "        str_true = \"Damaged\" if true_label == 1 else \"Normal\"\n",
        "        str_pred = \"Normal\" if true_label == 1 else \"Damaged\" # ì˜¤íƒì´ë¯€ë¡œ ë°˜ëŒ€\n",
        "        fail_type = \"False Negative (ë¯¸íƒ)\" if true_label == 1 else \"False Positive (ì˜¤íƒ/ê³¼íƒ)\"\n",
        "\n",
        "        plt.subplot(len(target_samples), 1, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"[{start_index+i+1}] {filename}\\nType: {fail_type} | GT: {str_true} (Green) vs Pred: {str_pred} (Red)\",\n",
        "                  fontsize=14, color='red', fontweight='bold')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZi4W9iMT01z"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8x_damage_stage2_4th', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_4th_test_results_conf025.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj5HlJPzT01z"
      },
      "source": [
        "## ì„±ëŠ¥ í…ŒìŠ¤íŠ¸(conf 0.25->0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKnBlScMT01z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# [Cell 3] ì„±ëŠ¥ í‰ê°€ (Detection & Classification)\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/FINE_TUNING_MODEL/yolov8x_damage_stage2_4th/weights/best.pt'\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "os.makedirs(RESULT_ROOT, exist_ok=True)\n",
        "\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/DATA/DAMAGE_DETECTION_CLASS_ALL\"\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION\"\n",
        "# YAML íŒŒì¼ ê²½ë¡œ\n",
        "YAML_PATH = os.path.join(DATASET_ROOT, 'data_damage_v2.yaml')\n",
        "\n",
        "def evaluate_performance(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
        "    print(f\"   - ëª¨ë¸: {model_path}\")\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ (Detection Metrics: mAP)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\nğŸ“Š [1] BBox íƒì§€ ì„±ëŠ¥ (mAP) ê³„ì‚° ì¤‘...\")\n",
        "    # split='test'ë¡œ ì„¤ì •í•˜ì—¬ Test Setì— ëŒ€í•´ í‰ê°€\n",
        "    metrics = model.val(data=YAML_PATH, split='test', verbose=False)\n",
        "\n",
        "    map50 = metrics.box.map50\n",
        "    map5095 = metrics.box.map\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ & ì†ë„ í‰ê°€ (Classification & Speed)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"ğŸ“Š [2] ì´ë¯¸ì§€ ë¶„ë¥˜ ì„±ëŠ¥ ë° ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...\")\n",
        "\n",
        "    TEST_IMG_DIR = os.path.join(DATASET_ROOT, 'images', 'test')\n",
        "    TEST_LBL_DIR = os.path.join(DATASET_ROOT, 'labels', 'test')\n",
        "\n",
        "    image_files = [f for f in os.listdir(TEST_IMG_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    results_list = []\n",
        "    speed_stats = []\n",
        "\n",
        "    for file in tqdm(image_files, desc=\"Inference\"):\n",
        "        img_path = os.path.join(TEST_IMG_DIR, file)\n",
        "        txt_file = os.path.splitext(file)[0] + \".txt\"\n",
        "        lbl_path = os.path.join(TEST_LBL_DIR, txt_file)\n",
        "\n",
        "        # A. ì •ë‹µ í™•ì¸ (ë¼ë²¨ íŒŒì¼ ë‚´ìš© ìœ ë¬´)\n",
        "        true_label = 0\n",
        "        if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
        "            true_label = 1 # Damaged\n",
        "\n",
        "        # B. ëª¨ë¸ ì¶”ë¡ \n",
        "        results = model(img_path, conf=0.1, verbose=False)\n",
        "\n",
        "        # C. ì†ë„ ì¸¡ì •\n",
        "        speed_info = results[0].speed\n",
        "        total_time_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']\n",
        "        speed_stats.append(total_time_ms)\n",
        "\n",
        "        # D. ì˜ˆì¸¡ ë¼ë²¨ (ë°•ìŠ¤ ìœ ë¬´)\n",
        "        pred_label = 0\n",
        "        if len(results[0].boxes) > 0:\n",
        "            pred_label = 1 # Damaged\n",
        "\n",
        "        results_list.append({\n",
        "            \"filename\": file,\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label,\n",
        "            \"is_correct\": (true_label == pred_label)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results_list)\n",
        "\n",
        "    # =========================================================\n",
        "    # [ìµœì¢… ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸]\n",
        "    # =========================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š [ìµœì¢… ë¶„ì„ ê²°ê³¼ - Fine-tuned Model]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. mAP ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"ğŸ¯ 1. ê°ì²´ íƒì§€ ì„±ëŠ¥ (Detection Metrics):\")\n",
        "    print(f\"   - mAP@50    : {map50:.4f}\")\n",
        "    print(f\"   - mAP@50-95 : {map5095:.4f}\")\n",
        "\n",
        "    # 2. ì •í™•ë„ ì¶œë ¥\n",
        "    acc = accuracy_score(df['true_label'], df['pred_label'])\n",
        "    print(f\"\\nâœ… 2. ë¶„ë¥˜ ì •í™•ë„ (Accuracy): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "    # 3. ì†ë„ ì¶œë ¥\n",
        "\n",
        "    if speed_stats:\n",
        "        avg_time = np.mean(speed_stats)\n",
        "        min_time = np.min(speed_stats)\n",
        "        max_time = np.max(speed_stats)\n",
        "        fps = 1000 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nâš¡ 3. ì¶”ë¡  ì†ë„ (Inference Speed):\")\n",
        "        print(f\"   - í‰ê·  ì†Œìš” ì‹œê°„ : {avg_time:.2f} ms/ì¥\")\n",
        "        print(f\"   - ìµœì†Œ ì†Œìš” ì‹œê°„ : {min_time:.2f} ms\")\n",
        "        print(f\"   - ìµœëŒ€ ì†Œìš” ì‹œê°„ : {max_time:.2f} ms\")\n",
        "        print(f\"   - ì²˜ë¦¬ëŸ‰ (FPS)   : {fps:.2f} FPS\")\n",
        "\n",
        "    # 4. ìƒì„¸ ë¦¬í¬íŠ¸\n",
        "    print(f\"\\nğŸ“ 4. ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "    print(classification_report(df['true_label'], df['pred_label'], target_names=['Normal', 'Damaged']))\n",
        "\n",
        "    # 5. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    cm = confusion_matrix(df['true_label'], df['pred_label'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred: Normal', 'Pred: Damaged'],\n",
        "                yticklabels=['True: Normal', 'True: Damaged'])\n",
        "    plt.title(f'Confusion Matrix\\n(Damage Detection)')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    save_path = os.path.join(RESULT_ROOT, \"inference_finetuned_4th_test_results_conf010.csv\")\n",
        "    df.to_csv(save_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\nğŸ’¾ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "evaluate_performance(best_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZDuy0CT010"
      },
      "source": [
        "## ì˜¤íƒ ëŒ€ìƒ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyS0OBpTT011"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë° ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
        "SAVE_DIR = os.path.join(PROJECT_ROOT, \"FINE_TUNING_MODEL\")\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'yolov8x_damage_stage2_4th', 'weights', 'best.pt')\n",
        "\n",
        "RESULT_ROOT = \"/content/drive/MyDrive/03. HDMF/(pre_study)2026_HDMF_AUTO_SPOKE/SUBJECT/WEEK2_DAMAGE_DETECTION/RESULT\"\n",
        "RESULT_CSV_PATH = os.path.join(RESULT_ROOT, \"inference_finetuned_4th_test_results_conf010.csv\")\n",
        "\n",
        "visualize_failures_batch(start_index=0, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1FKcXbPT011"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}